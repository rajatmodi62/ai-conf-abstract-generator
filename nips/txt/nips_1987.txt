Bit-Serial Neural Networks
Alan Murray, Anthony Smith, Zoe Butler
A  bit  - serial  VLSI  neural  network  is  described  from  an  initial  architecture  for  a  synapse array through to silicon layout and board design.  The issues surrounding bit  - serial  computation,  and  analog/digital  arithmetic  are  discussed  and  the  parallel  development  of  a  hybrid  analog/digital  neural  network  is  outlined.  Learning  and  recall  capabilities  are  reported  for  the  bit  - serial  network  along  with  a  projected  specification  for  a  64  - neuron,  bit  - serial  board  operating  at 20 MHz.  This tech(cid:173) nique  is  extended  to  a  256  (2562  synapses)  network  with  an  update  time  of 3ms,  using  a  "paging"  technique  to  time  - multiplex  calculations  through  the  synapse  array.
************************************
Connectivity Versus Entropy
Yaser Abu-Mostafa
How  does  the  connectivity  of a  neural  network  (number  of synapses  per  neuron)  relate  to  the complexity  of the  problems  it  can  handle  (measured  by  the entropy)?  Switching theory would suggest no relation at all, since all Boolean  functions  can be  implemented  using  a  circuit  with very  low  connectivity  (e.g.,  using  two-input  NAND  gates).  However,  for  a  network  that  learns  a  problem  from  examples  using  a  local  learning  rule,  we  prove  that  the  entropy  of  the  problem becomes  a  lower  bound for  the connectivity of the network.
************************************
The Hopfield Model with Multi-Level Neurons
Michael Fleisher
The  Hopfield  neural  network.  model  for  associative  memory  is  generalized.  The  generalization 
************************************
How Neural Nets Work
Alan Lapedes, Robert Farber
There is  presently great interest in the abilities of neural networks to mimic 
"qualitative reasoning"  by manipulating neural incodings of symbols.  Less work 
has  been performed on using neural networks to process floating  point numbers 
and it is  sometimes stated that neural networks are somehow inherently inaccu(cid:173)
rate  and  therefore  best  suited  for  "fuzzy"  qualitative reasoning.  Nevertheless, 
the  potential  speed  of massively  parallel  operations  make  neural  net  "number 
crunching"  an interesting topic  to explore.  In this paper we  discuss some of our 
work in which we  demonstrate that for  certain applications neural networks can 
achieve  significantly  higher  numerical  accuracy  than  more  conventional  tech(cid:173)
niques.  In  particular,  prediction  of future  values  of a  chaotic  time  series  can 
be  performed  with  exceptionally  high  accuracy.  We  analyze  how  a  neural  net 
is  able  to do  this  ,  and in  the process  show  that  a  large class  of functions  from 
Rn.  ~ Rffl  may  be  accurately  approximated  by  a  backpropagation  neural  net 
with just two  "hidden"  layers.  The network  uses  this functional  approximation 
to perform either interpolation (signal processing applications)  or extrapolation 
(symbol processing applicationsJ.  Neural nets therefore use quite familiar meth(cid:173)
ods to perform. their tasks.  The geometrical viewpoint advocated here seems to 
be a  useful  approach  to analyzing  neural  network  operation  and  relates  neural 
networks  to well  studied topics  in  functional  approximation.
************************************
Spatial Organization of Neural Networks: A Probabilistic Modeling Approach
Andreas Stafylopatis, Marios Dikaiakos, D. Kontoravdis
The  aim  of  this  paper  is  to  explore  the  spatial  organization  of  neural  networks  under  Markovian  assumptions,  in  what  concerns  the be(cid:173) haviour  of  individual  cells  and  the  interconnection  mechanism.  Space(cid:173) organizational  properties  of  neural  nets  are  very  relevant  in  image  modeling  and  pattern  analysis,  where  spatial  computations  on  stocha(cid:173) stic  two-dimensional  image  fields  are  involved.  As  a  first  approach  we  develop  a  random  neural  network  model,  based  upon  simple  probabi(cid:173) listic  assumptions,  whose  organization  is  studied  by  means  of  dis(cid:173) crete-event  simulation.  We  then  investigate  the  possibility  of  ap(cid:173) proXimating  the  random  network's  behaviour  by  using  an  analytical  ap(cid:173) proach  originating  from  the  theory  of  general  product-form  queueing  networks.  The  neural  network  is  described  by  an  open  network  of  no(cid:173) des,  in  which  customers  moving  from  node  to  node  represent  stimula(cid:173) tions  and  connections  between  nodes  are  expressed  in  terms  of  sui(cid:173) tably  selected  routing  probabilities.  We  obtain  the  solution  of  the  model  under  different  disciplines  affecting  the  time  spent  by  a  sti(cid:173) mulation  at  each  node  visited.  Results  concerning  the  distribution  of  excitation  in  the  network  as  a  function  of  network  topology  and  external  stimulation  arrival  pattern  are  compared  with  measures  ob(cid:173) tained  from  the  simulation  and  validate  the  approach  followed.
************************************
A Neural-Network Solution to the Concentrator Assignment Problem
Gene Tagliarini, Edward Page
Networks  of simple analog  processors  having  neuron-like properties have  been  employed  to  compute  good  solutions  to  a  variety  of optimization  prob(cid:173) lems.  This  paper presents  a  neural-net solution to  a  resource allocation prob(cid:173) lem that arises  in  providing  local  access  to  the  backbone of a  wide-area  com(cid:173) munication  network.  The  problem is  described in  terms of an energy function  that can be  mapped onto an analog computational network.  Simulation results  characterizing  the  performance  of the  neural  computation  are  also  presented.
************************************
LEARNING BY STATE RECURRENCE DETECTION
Bruce Rosen, James Goodwin, Jacques Vidal
This research investigates a new technique for unsupervised learning of nonlinear  control problems. The approach is applied both to Michie and Chambers BOXES  algorithm and to Barto, Sutton and Anderson's extension, the ASE/ACE system, and  has significantly improved the convergence rate of stochastically based learning  automata. 
************************************
Stability Results for Neural Networks
Anthony Michel, Jay Farrell, Wolfgang Porod
In the present paper we survey and utilize results from the qualitative theory of large  scale interconnected dynamical systems in order to develop  a  qualitative theory for  the  Hopfield model of neural networks.  In our approach we  view such networks as  an inter(cid:173) connection of many single neurons.  Our results  are  phrased in  terms of the  qualitative  properties of the individual neurons and in terms of the properties of the interconnecting  structure of the neural  networks.  Aspects of neural networks which  we  address include  asymptotic stability,  exponential stability,  and instability  of an  equilibrium;  estimates  of trajectory bounds; estimates of the domain of attraction of an asymptotically stable  equilibrium;  and stability of neural networks  under structural perturbations.
************************************
Introduction to a System for Implementing Neural Net Connections on SIMD Architectures
Sherryl Tomboulian
Neural  networks  have  attracted  much  interest  recently,  and  using  parallel 
************************************
Optimization with Artificial Neural Network Systems: A Mapping Principle and a Comparison to Gradient Based Methods
Harrison Leong
General  formulae  for  mapping  optimization  problems  into  systems  of  ordinary  differential 
************************************
Optimal Neural Spike Classification
James Bower, Amir Atiya
Being able  to record the electrical activities of a  number of neurons simultaneously is  likely  to be important in  the study of the functional organization of networks of real neurons.  Using  one  extracellular  microelectrode  to  record  from  several neurons  is  one  approach  to  studying  the  response  properties  of sets  of  adjacent  and  therefore  likely  related  neurons.  However,  to  do  this,  it  is  necessary  to  correctly  classify  the  signals  generated  by  these  different  neurons.  This paper considers  this problem of classifying the  signals  in  such an  extracellular recording,  based upon their shapes, and specifically considers the classification of signals in the case when  spikes overlap  temporally.
************************************
REFLEXIVE ASSOCIATIVE MEMORIES
Hendricus G. Loos
In the synchronous discrete model, the average memory capacity of  bidirectional associative memories (BAMs) is compared with that of  Hopfield memories, by means of a calculat10n of the percentage of good  recall for 100 random BAMs of dimension 64x64, for different numbers  of stored vectors. The memory capac1ty Is found to be much smal1er than  the Kosko upper bound, which Is the lesser of the two dimensions of the  BAM. On the average, a 64x64 BAM has about 68 % of the capacity of the  corresponding Hopfield memory with the same number of neurons. Ortho(cid:173) normal coding of the BAM Increases the effective storage capaCity by  only 25 %. The memory capacity limitations are due to spurious stable  states, which arise In BAMs In much the same way as in Hopfleld  memories. Occurrence of spurious stable states can be avoided by  replacing the thresholding in the backlayer of the BAM by another  nonl1near process, here called "Dominant Label Selection" (DLS). The  simplest DLS is the wlnner-take-all net, which gives a fault-sensitive  memory. Fault tolerance can be improved by the use of an orthogonal or  unitary transformation. An optical application of the latter is a Fourier  transform, which is implemented simply by a lens.
************************************
The Performance of Convex Set Projection Based Neural Networks
Robert Marks, Les Atlas, Seho Oh, James Ritcey
and
************************************
Speech Recognition Experiments with Perceptrons
David Burr
Artificial  neural  networks  (ANNs)  are  capable  of accurate  recognition  of  simple speech  vocabularies such  as  isolated  digits  [1].  This paper looks  at two  more  difficult  vocabularies,  the  alphabetic  E-set  and  a  set  of  polysyllabic  words.  The  E-set  is  difficult  because  it  contains  weak  discriminants  and  polysyllables  are  difficult  because  of  timing  variation.  Polysyllabic  word  recognition  is  aided  by a  time  pre-alignment technique  based on  dynamic pro(cid:173) gramming  and  E-set  recognition  is  improved  by  focusing  attention.  Recogni(cid:173) tion  accuracies  are  better  than  98%  for  both  vocabularies  when  implemented  with a single  layer perceptron.
************************************
On Properties of Networks of Neuron-Like Elements
Pierre Baldi, Santosh Venkatesh
The  complexity  and  computational  capacity  of multi-layered,  feedforward  neural networks is examined.  Neural networks for special purpose (structured)  functions  are examined  from  the  perspective of circuit  complexity.  Known  re(cid:173) sults in  complexity theory are applied to the special instance of neural network  circuits,  and  in  particular,  classes  of functions  that  can  be  implemented  in  shallow circuits characterised.  Some conclusions are  also  drawn about learning  complexity,  and some  open problems raised.  The dual  problem of determining  the computational capacity of a  class of multi-layered  networks with  dynamics  regulated  by  an  algebraic  Hamiltonian  is  considered.  Formal  results  are  pre(cid:173) sented  on  the  storage  capacities  of programmed  higher-order  structures,  and  a  tradeoff between ease  of programming  and  capacity is  shown.  A  precise  de(cid:173) termination is  made of the static fixed  point structure of random higher-order  constructs,  and phase-transitions (0-1  laws)  are  shown.
************************************
Ensemble' Boltzmann Units have Collective Computational Properties like those of Hopfield and Tank Neurons
Mark Derthick, Joe Tebelskis
Requests for name changes in the electronic proceedings will be accepted with no questions asked.  However name changes may cause bibliographic tracking issues.  Authors are asked to consider this carefully and discuss it with their co-authors prior to requesting a name change in the electronic proceedings.
************************************
On Tropistic Processing and Its Applications
Manuel Fernández
The  interaction  of  a  set  of  tropisms  is  sufficient  in  many 
************************************
Neuromorphic Networks Based on Sparse Optical Orthogonal Codes
Mario Vecchi, Jawad Salehi
Requests for name changes in the electronic proceedings will be accepted with no questions asked.  However name changes may cause bibliographic tracking issues.  Authors are asked to consider this carefully and discuss it with their co-authors prior to requesting a name change in the electronic proceedings.
************************************
A 'Neural' Network that Learns to Play Backgammon
Gerald Tesauro, Terrence J. Sejnowski
We describe a class of connectionist networks that have learned to play back(cid:173)
************************************
Learning Representations by Recirculation
Geoffrey E. Hinton, James McClelland
We describe a new learning procedure for networks that contain groups of non(cid:173)
************************************
A Computer Simulation of Cerebral Neocortex: Computational Capabilities of Nonlinear Neural Networks
Alexander Singer, John Donoghue
Requests for name changes in the electronic proceedings will be accepted with no questions asked.  However name changes may cause bibliographic tracking issues.  Authors are asked to consider this carefully and discuss it with their co-authors prior to requesting a name change in the electronic proceedings.
************************************
PATTERN CLASS DEGENERACY IN AN UNRESTRICTED STORAGE DENSITY MEMORY
Christopher Scofield, Douglas L. Reilly, Charles Elbaum, Leon Cooper
in
************************************
Strategies for Teaching Layered Networks Classification Tasks
Ben Wittner, John Denker
There is  a widespread misconception  that the delta-rule is in some sense guaranteed to  work  on  networks  without hidden units.  As  previous authors have mentioned,  there is  no such guarantee for  classification tasks.  We  will begin by  presenting explicit counter(cid:173) examples illustrating two different interesting ways in which  the delta rule can fail.  We  go on  to provide conditions  which  do guarantee  that gradient  descent  will  successfully  train  networks  without  hidden  units  to  perform  two-category  classification  tasks.  We  discuss  the  generalization  of our  ideas  to  networks  with  hidden  units  and  to  multi(cid:173) category classification  tasks. 
************************************
Invariant Object Recognition Using a Distributed Associative Memory
Harry Wechsler, George Zimmerman
This  paper  describes  an  approach  to  2-dimensional  object  recognition.  Complex-log  con(cid:173) formal  mapping  is  combined  with  a  distributed  associative  memory  to  create  a  system  which  recognizes  objects  regardless  of  changes  in  rotation  or  scale.  Recalled  information  from  the  memorized  database  is  used  to  classify  an object,  reconstruct  the  memorized  ver(cid:173) sion  of the  object,  and estimate  the  magnitude of changes in  scale  or rotation.  The system  response  is  resistant  to  moderate  amounts of noise  and occlusion.  Several experiments,  us(cid:173) ing real,  gray scale images,  are  presented to show  the feasibility  of our approach.
************************************
Cycles: A Simulation Tool for Studying Cyclic Neural Networks
Michael Gately
A computer program has been designed and implemented to allow a researcher 
************************************
Learning on a General Network
Amir Atiya
This paper generalizes the backpropagation method to a  general network containing feed(cid:173)
************************************
Neural Net and Traditional Classifiers
William Huang, Richard P. Lippmann
Abstract.  Previous  work on  nets  with  continuous-valued inputs  led  to generative 
procedures  to construct convex decision  regions with two-layer perceptrons (one hidden 
layer) and  arbitrary decision  regions  with  three-layer perceptrons  (two hidden layers). 
Here we demonstrate that two-layer perceptron classifiers trained with back propagation 
can  form  both  convex  and  disjoint  decision  regions.  Such  classifiers  are  robust,  train 
rapidly,  and  provide  good  performance  with  simple  decision  regions.  When  complex 
decision  regions  are  required,  however,  convergence  time  can  be  excessively  long  and 
performance is  often no better than that of k-nearest  neighbor classifiers.  Three neural 
net  classifiers  are  presented  that  provide  more  rapid  training  under  such  situations. 
Two use  fixed  weights in  the  first  one  or  two layers and  are  similar  to classifiers  that 
estimate probability density functions using histograms.  A third "feature map classifier" 
uses  both  unsupervised  and  supervised  training.  It provides  good  performance  with 
little supervised training in situations such as speech recognition where much unlabeled 
training data is  available.  The architecture of this classifier can  be  used  to implement 
a  neural net  k-nearest  neighbor classifier.
************************************
Scaling Properties of Coarse-Coded Symbol Memories
Ronald Rosenfeld, David Touretzky
Abstract:  Coarse-coded symbol memories have appeared in several neural network 
symbol  processing  models.  In  order  to  determine  how  these  models  would  scale,  one 
must  first  have  some  understanding  of the  mathematics  of coarse-coded  representa(cid:173)
tions.  We  define  the  general  structure  of coarse-coded  symbol  memories  and  derive 
mathematical relationships among  their essential parameters:  memory 8ize,  8ymbol-8et 
size and capacity.  The computed capacity of one of the schemes agrees well with actual 
measurements oC  tbe coarse-coded working memory of DCPS, Touretzky and Hinton's 
distributed connectionist production system.
************************************
Synchronization in Neural Nets
Jacques Vidal, John Haggerty
The  paper  presents  an  artificial  neural  network  concept  (the  Synchronizable Oscillator Networks)  where the instants of individual  firings  in  the  form  of  point  processes  constitute  the  only  form  of  information  transmitted  between  joining  neurons.  This  type  of  communication contrasts with  that which  is  assumed  in most  other  models  which  typically  are  continuous  or  discrete  value-passing  networks.  Limiting the messages received  by each processing unit to  time  markers that signal  the firing  of other units  presents  significant  implemen tation advantages. 
************************************
A NEURAL NETWORK CLASSIFIER BASED ON CODING THEORY
Tzi-Dar Chiueh, Rodney Goodman
The new neural network classifier we propose transforms the  classification problem into the coding theory problem of decoding a noisy  codeword. An input vector in the feature space is transformed into an internal  representation which is a codeword in the code space, and then error correction  decoded in this space to classify the input feature vector to its class. Two classes  of codes which give high performance are the Hadamard matrix code and the  maximal length sequence code. We show that the number of classes stored in an  N-neuron system is linear in N and significantly more than that obtainable by  using the Hopfield type memory as a classifier.
************************************
Microelectronic Implementations of Connectionist Neural Networks
Stuart Mackie, Hans Graf, Daniel Schwartz, John Denker
In  this  paper  we  discuss  why  special  purpose  chips  are  needed  for  useful  implementations  of connectionist  neural  networks  in  such  applications  as  pattern  recognition  and  classification.  Three  chip  designs  are  described:  a  hybrid  digital/analog programmable  connection  matrix,  an  analog  connection  matrix  with  adjustable connection strengths, and a digital pipe lined best-match chip.  The common  feature  of the designs  is the distribution of arithmetic processing power amongst the  data storage to minimize data movement.
************************************
Analysis of Distributed Representation of Constituent Structure in Connectionist Systems
Paul Smolensky
A  general  method, the  tensor product representation, is described for  the distributed representation of  value/variable bindings.  The method allows the fully distributed representation of symbolic structures:  the roles  in  the structures, as well as the fillers  for  those roles, can be arbitrarily non-local.  Fully and  partially localized  special cases reduce to existing cases of connectionist representations of structured  data;  the  tensor  product  representation  generalizes  these  and  the  few  existing  examples  of  fuUy  distributed  representations  of structures.  The  representation  saturates  gracefully  as  larger  structures  are  represented;  it penn its  recursive  construction  of complex  representations  from  simpler  ones;  it  respects  the  independence  of the  capacities  to generate and  maintain  multiple bindings in  parallel;  it  extends naturally to continuous structures and continuous representational patterns; it pennits values to  also  serve  as  variables;  it  enables  analysis  of  the  interference  of  symbolic  structures  stored  in  associative  memories;  and  it  leads  to characterization  of optimal  distributed representations  of roles  and a recirculation algorithm for learning them.
************************************
Hierarchical Learning Control - An Approach with Neuron-Like Associative Memories
Enis Ersü, Henning Tolle
Advances
************************************
Presynaptic Neural Information Processing
L. Carley
The  potential  for  presynaptic  information  processing  within  the  arbor  of a  single  axon  will  be  discussed  in  this  paper.  Current  knowledge  about  the  activity  dependence  of  the  firing  threshold,  the  conditions  required  for  conduction  failure,  and  the  similarity  of  nodes  along  a  single  axon  will  be  reviewed.  An  electronic  circuit  model  for  a  site  of low  conduction  safety  in  an  axon  will  be  presented.  In  response to  single  frequency  stimulation  the  electronic circuit acts  as  a  lowpass filter.
************************************
An Optimization Network for Matrix Inversion
Ju-Seog Jang, Soo-Young Lee, Sang-Yung Shin
Inverse  matrix  calculation  can  be  considered  as  an  optimization.  We  have  demonstrated  that  this  problem  can  be  rapidly  solved  by  highly  interconnected  simple  neuron-like  analog  processors.  A  network  for  matrix  inversion  based  on  the  concept  of  Hopfield's  neural  network  was  designed,  and  implemented  with  electronic  hardware.  With  slight  modifications,  the  network  is  readily  applicable  to  solving  a  linear  simultaneous  equation  efficiently.  Notable  features  of  this  circuit  are  potential  speed  due  to  parallel  processing,  and  robustness  against  variations  of  device  parameters.
************************************
Basins of Attraction for Electronic Neural Networks
Charles Marcus, R. Westervelt
We  have  studied  the  basins  of  attraction  for  fixed  point  and  oscillatory  attractors  in  an  electronic  analog  neural  network.  Basin  measurement  circuitry  periodically  opens  the  network  feedback  loop,  loads  raster-scanned  initial  conditions  and  examines  the  resulting  attractor.  Plotting  the  basins  for  fixed  points  (memories),  we  show  that  overloading  an  associative  memory  network  leads  to  irregular  basin  shapes.  The  network  also  includes  analog  time  delay  circuitry,  and  we  have  shown  that  delay  in  symmetric  networks  can  introduce  basins  for  oscillatory  attractors.  Conditions  leading  to  oscillation  are  related  to  the  presence  of  frustration;  reducing  frustration  by  diluting  the  connections  can  stabilize  a  delay  network.
************************************
Programmable Synaptic Chip for Electronic Neural Networks
Alexander Moopenn, H. Langenbacher, A. Thakoor, S. Khanna
A binary  synaptic  matrix  chip  has  been  developed  for  electronic  neural  networks.  The  matrix  chip  contains  a  programmable  32X32  array  of  "long  channel"  NMOSFET  binary  connection  elements  imple(cid:173) mented  in  a  3-um  bulk  CMOS  process.  Since  the  neurons  are  kept  off(cid:173) chip,  the  synaptic  chip  serves  as  a  "cascadable"  building  block  for  a  multi-chip  synaptic  network  as  large  as  512X512  in  size.  As  an  alternative  the  programmable  NMOSFET  (long  channel)  connection  elements,  tailored  thin  film  resistors  are  deposited,  in  series  with  FET  switches,  on  some  CMOS  test  chips,  to  obtain  the  weak  synaptic  connections.  Although  deposition  and  patterning  of  the  resistors  require  additional  they  promise  substantial  savings  in  silcon  area.  The  performance  of  a  synaptic  chip  in  a  32- neuron  breadboard  system  in  an  associative  memory  test  application  is  discussed. 
************************************
Learning a Color Algorithm from Examples
Tomaso A. Poggio, Anya Hurlbert
A lightness algorithm that separates surface reflectance from illumination in a  Mondrian world is synthesized automatically from a set of examples, pairs of input  (image irradiance) and desired output (surface reflectance). The algorithm, which re(cid:173) sembles a new lightness algorithm recently proposed by Land, is approximately equiva(cid:173) lent to filtering the image through a center-surround receptive field in individual chro(cid:173) matic channels. The synthesizing technique, optimal linear estimation, requires only  one assumption, that the operator that transforms input into output is linear. This  assumption is true for a certain class of early vision algorithms that may therefore be  synthesized in a similar way from examples. Other methods of synthesizing algorithms  from examples, or "learning", such as backpropagation, do not yield a significantly dif(cid:173) ferent or better lightness algorithm in the Mondrian world. The linear estimation and  backpropagation techniques both produce simultaneous brightness contrast effects. 
************************************
Generalization of Back propagation to Recurrent and Higher Order Neural Networks
Fernando Pineda
A general method for deriving backpropagation algorithms for networks 
************************************
Neural Network Implementation Approaches for the Connection Machine
Nathan Brown
The SIMD parallelism of the Connection Machine (eM) allows the construction of 
neural network simulations by the use of simple data and control structures.  Two 
approaches are described which allow parallel computation of a model's nonlinear 
functions, parallel modification of a model's weights, and parallel propagation of a 
model's activation and error.  Each approach also allows a model's interconnect 
structure to be physically dynamic.  A Hopfield model is implemented with each 
approach at six sizes over the same number of CM processors to provide a performance 
comparison.
************************************
On the Power of Neural Networks for Solving Hard Problems
Jehoshua Bruck, Joseph Goodman
This  paper deals  with a  neural network model in  which each neuron  performs a  threshold logic function.  An important property of the model  is  that  it always  converges  to a  stable state  when  operating in  a  serial  mode [2,5].  This property is  the basis of the potential applications of the  model such as associative memory devices and combinatorial optimization  [3,6].  One of the motivations for use of the model for solving hard combinatorial  problems  is  the fact  that it can  be implemented  by optical devices  and  thus operate at a  higher speed  than conventional electronics.  The main theme in this work is  to investigate the power of the model for  solving NP-hard problems  [4,8],  and to understand  the relation  between  speed of operation and the size of a  neural network.  In particular, it will  be  shown  that  for  any  NP-hard  problem  the  existence  of a  polynomial  size  network  that  solves  it  implies  that  NP=co-NP.  Also,  for  Traveling  Salesman  Problem  (TSP), even  a  polynomial  size  network  that  gets  an  €-approximate  solution does  not exist unless  P=NP. 
************************************
HOW THE CATFISH TRACKS ITS PREY: AN INTERACTIVE "PIPELINED" PROCESSING SYSTEM MAY DIRECT FORAGING VIA RETICULOSPINAL NEURONS
Jagmeet S. Kanwal
of
************************************
Phasor Neural Networks
André Noest
A novel  network  type  is  introduced  which  uses  unit-length  2-vectors 
************************************
Computing Motion Using Resistive Networks
Christof Koch, Jin Luo, Carver Mead, James Hutchinson
Requests for name changes in the electronic proceedings will be accepted with no questions asked.  However name changes may cause bibliographic tracking issues.  Authors are asked to consider this carefully and discuss it with their co-authors prior to requesting a name change in the electronic proceedings.
************************************
Experimental Demonstrations of Optical Neural Computers
Ken Hsu, David Brady, Demetri Psaltis
We  describe  two  expriments  in  optical  neural  computing.  In  the  first  a  closed  optical  feedback  loop  is  used  to  implement  auto-associative  image  recall.  In the second a perceptron-Iike learning algorithm is  implemented with  photorefractive holography.
************************************
MURPHY: A Robot that Learns by Doing
Bartlett Mel
MURPHY consists of a camera looking at a robot arm, with a connectionist network  architecture situated in between. By moving its arm through a small, representative  sample of the 1 billion possible joint configurations, MURPHY learns the relationships,  backwards and forwards, between the positions of its joints and the state of its visual field.  MURPHY can use its internal model in the forward direction to "envision" sequences  of actions for planning purposes, such as in grabbing a visually presented object, or in  the reverse direction to "imitate", with its arm, autonomous activity in its visual field.  Furthermore, by taking explicit advantage of continuity in the mappings between visual  space and joint space, MURPHY is able to learn non-linear mappings with only a single  layer of modifiable weights.
************************************
SPONTANEOUS AND  INFORMATION-TRIGGERED SEGMENTS OF SERIES OF HUMAN BRAIN ELECTRIC FIELD MAPS
D. Lehmann, D. Brandeis, A. Horst, H. Ozaki, I. Pal
The brain works in a state-dependent manner: processin9 
************************************
Simulations Suggest Information Processing Roles for the Diverse Currents in Hippocampal Neurons
Lyle Borg-Graham
A computer model of the hippocampal pyramidal cell (HPC) is  described 
************************************
An Artificial Neural Network for Spatio-Temporal Bipolar Patterns: Application to Phoneme Classification
Les Atlas, Toshiteru Homma, Robert Marks
An  artificial  neural  network  is  developed  to  recognize  spatio-temporal  bipolar patterns  associatively.  The  function  of a formal  neuron is  generalized by  replacing  multiplication  with  convolution,  weights  with  transfer  functions,  and  thresholding  with  nonlinear  transform  following  adaptation.  The Hebbian  learn(cid:173) ing  rule  and  the  delta  learning  rule  are  generalized  accordingly,  resulting  in  the  learning  of weights  and  delays.  The  neural  network  which  was  first  developed  for  spatial  patterns  was  thus  generalized  for  spatio-temporal  patterns.  It  was  tested  using  a  set  of bipolar input patterns  derived from  speech  signals,  showing  robust classification of 30 model phonemes.
************************************
Teaching Artificial Neural Systems to Drive: Manual Training Techniques for Autonomous Systems
J. F. Shepanski, S. A. Macy
Requests for name changes in the electronic proceedings will be accepted with no questions asked.  However name changes may cause bibliographic tracking issues.  Authors are asked to consider this carefully and discuss it with their co-authors prior to requesting a name change in the electronic proceedings.
************************************
Correlational Strength and Computational Algebra of Synaptic Connections Between Neurons
Eberhard Fetz
Intracellular  recordings  in  spinal  cord  motoneurons  and  cerebral  cortex neurons have provided new evidence on the correlational strength of  monosynaptic  connections,  and  the  relation  between  the  shapes  of  postsynaptic  potentials  and  the  associated  increased  firing  probability.  In  these  cells,  excitatory  postsynaptic  potentials  (EPSPs)  produce  cross(cid:173) correlogram peaks  which resemble  in large part the derivative of  the EPSP.  Additional  synaptic  noise broadens  the peak,  but the  peak  area  -- i.e.,  the  number of above-chance firings triggered per EPSP -- remains proportional to  the EPSP  amplitude.  A typical EPSP of 100  ~v triggers about .01  firings per  EPSP.  The  consequences  of  these  data  for  information  processing  by  polysynaptic connections is discussed.  The effects of sequential polysynaptic  links  can  be  calculated  by  convolving  the  effects  of  the  underlying  monosynaptic connections.  The net effect of parallel pathways is the sum of  the individual contributions.
************************************
Discovering Structure from Motion in Monkey, Man and Machine
Ralph Siegel
The ability to obtain three-dimensional structure from visual motion is 
************************************
Static and Dynamic Error Propagation Networks with Application to Speech Coding
A. Robinson, F. Fallside
Error propagation nets have been shown to be able to learn a variety of tasks in  which a static input pattern is mapped outo a static output pattern. This paper  presents a generalisation of these nets to deal with time varying, or dynamic  patterns, and three possible architectures are explored. As an example, dynamic  nets are applied to tbe problem of speech coding, in which a time sequence of  speech data are coded by one net and decoded by another. The use of dynamic  nets gives a better signal to noise ratio than that achieved using static nets.
************************************
Schema for Motor Control Utilizing a Network Model of the Cerebellum
James Houk
This  paper  outlines  a  schema  for  movement  control 
************************************
Distributed Neural Information Processing in the Vestibulo-Ocular System
Clifford Lau, Vicente Honrubia
A new distributed neural information-processing 
************************************
Time-Sequential Self-Organization of Hierarchical Neural Networks
Ronald Silverman, Andrew Noetzel
Self-organization  of  multi-layered  networks  can  be  realized  time-sequential  organization  of  successive  neural  layers.  by  Lateral  inhibition  operating  in  the  surround  of  firing  cells  in  each  for  unsupervised  capture  of  excitation  patterns  presented  by  the  previous  layer.  By  presenting  patterns  of  organization,  higher  implicit  in  the  pattern  set. 
************************************
A Method for the Design of Stable Lateral Inhibition Networks that is Robust in the Presence of Circuit Parasitics
John Wyatt, D. Standley
In  the  analog  VLSI  implementation  of  neural  systems,  it is 
************************************
Constrained Differential Optimization
John Platt, Alan Barr
Many optimization models of neural  networks need constraints to restrict the space of outputs to  a subspace which satisfies external criteria.  Optimizations using energy methods yield "forces" which  act upon  the  state of the  neural  network.  The penalty method, in which quadratic  energy  constraints  are  added  to  an  existing  optimization  energy,  has  become  popular  recently,  but  is  not  guaranteed  to satisfy  the  constraint conditions  when  there  are  other forces  on  the  neural  model  or when  there  are  multiple constraints.  In this paper, we present the basic differential multiplier method (BDMM),  which  satisfies constraints exactly;  we  create forces  which gradually apply  the constraints over time,  using "neurons" that estimate Lagrange multipliers. 
************************************
Encoding Geometric Invariances in Higher-Order Neural Networks
C. Giles, R. Griffin, T. Maxwell
We  describe  a  method  of  constructing  higher-order  neural 
************************************
A Novel Net that Learns Sequential Decision Process
Guo-Zheng Sun, Yee-Chun Lee, Hsing-Hen Chen
We propose a  new  scheme  to construct  neural networks  to classify  pat(cid:173)
************************************
Mathematical Analysis of Learning Behavior of Neuronal Models
John Cheung, Massoud Omidvar
In  this  paper,  we  wish  to  analyze  the  convergence  behavior  of a  number  of neuronal plasticity models.  Recent neurophysiological research suggests that  the neuronal behavior is adaptive.  In particular, memory stored within a neuron  is  associated with the synaptic weights which are varied or adjusted to achieve  learning.  A  number  of adaptive  neuronal  models  have  been  proposed  in  the  literature.  Three specific models will be analyzed in this paper, specifically the  Hebb model, the Sutton-Barto model, and the most recent trace model.  In this  paper we  will  examine  the conditions  for  convergence,  the  position  of conver(cid:173) gence and the rate at convergence,  of these models  as they applied to classical  conditioning.  Simulation results  are also presented to verify the analysis.
************************************
New Hardware for Massive Neural Networks
Darryl Coon, A. Perera
Transient phenomena associated with forward biased silicon p + - n - n + struc(cid:173) tures at 4.2K show remarkable similarities with biological neurons.  The devices  play  a  role  similar to the  two-terminal switching elements in  Hodgkin-Huxley  equivalent  circuit  diagrams.  The  devices  provide simpler  and  more  realistic  neuron  emulation  than  transistors  or op-amps.  They  have  such  low  power  and  current  requirements  that  they  could  be  used  in  massive  neural  networks.  Some  observed  properties  of  simple  circuits  containing  the  devices  include  action  potentials,  refractory  periods,  threshold behavior, excitation, inhibition, summation over synaptic inputs, synaptic  weights,  temporal  integration, memory,  network  connectivity modification  based  on  experience, pacemaker activity, firing  thresholds, coupling to sensors with graded sig(cid:173) nal  outputs  and  the  dependence  of firing  rate  on input  current.  Transfer functions  for simple artificial neurons with spiketrain inputs and spiketrain outputs have been  measured  and correlated with input coupling.
************************************
An Adaptive and Heterodyne Filtering Procedure for the Imaging of Moving Objects
F. Schuling, H. Mastebroek, W. Zaagman
Recent experimental work on the stimulus velocity dependent time resolving  power of the neural units, situated in the highest order optic ganglion of the  blowfly, revealed the at first sight amazing phenomenon that at this high level of  the fly visual system, the time constants of these units which are involved in the  processing of neural activity evoked by moving objects, are -roughly spoken(cid:173) inverse proportional to the velocity of those objects over an extremely wide range.  In this paper we will discuss the implementation of a two dimensional heterodyne  adaptive filter construction into a computer simulation model. The features of this  simulation model include the ability to account for the experimentally observed  stimulus-tuned adaptive temporal behaviour of time constants in the fly visual  system. The simulation results obtained, clearly show that the application of such  an adaptive processing procedure delivers an improved imaging technique of  moving patterns in the high velocity range. 
************************************
Phase Transitions in Neural Networks
Joshua Chover
Various  simulat.ions  of  cort.ical  subnetworks  have  evidenced  something  like  phase  transitions  with  respect  to  key  parameters.  We  demonstrate  that.  such  transi t.ions  must.  indeed  exist.  in  analogous  infinite  array  models.  For  related  finite  array  models  classical  phase  transi t.ions  (which  describe  steady-state  behavior)  may  not.  exist.,  but.  there  can  be  distinct. quali tative  changes  in  ("metastable")  transient  behavior  as  key  system  parameters  pass  through  crit.ical  values .
************************************
Using Neural Networks to Improve Cochlear Implant Speech Perception
Manoel Tenorio
-
************************************
Self-Organization of Associative Database and Its Applications
Hisashi Suzuki, Suguru Arimoto
An  efficient  method  of self-organizing  associative  databases  is  proposed  together  with  applications  to  robot  eyesight  systems.  The  proposed  databases  can  associate  any  input  with  some  output.  In  the  first  half part  of discussion,  an  algorithm of self-organization  is  proposed.  From  an  aspect  of  hardware,  it  produces  a  new  style  of  neural  network.  In  the  latter half part, an applicability to handwritten letter recognition and that to an autonomous  mobile  robot system are demonstrated.
************************************
Temporal Patterns of Activity in Neural Networks
Paolo Gaudiano
Patterns  of activity  over  real  neural  structures  are  known  to  exhibit  time(cid:173)
************************************
Network Generality, Training Required, and Precision Required
John Denker, Ben Wittner
We  show  how  to estimate  (1)  the  number  of functions  that  can  be implemented  by  a  particular  network  architecture,  (2)  how  much  analog  precision  is  needed  in  the  con(cid:173) nections in the network, and (3) the number of training examples the network must see  before it can  be expected  to form  reliable  generalizations. 
************************************
High Order Neural Networks for Efficient Associative Memory Design
Gérard Dreyfus, Isabelle Guyon, Jean-Pierre Nadal, Léon Personnaz
We  propose  learning  rules  for  recurrent  neural  networks  with  high-order  interactions  between  some or all  neurons.  The designed  networks  exhibit the  desired associative  memory  function: perfect  storage  and  retrieval  of pieces  of information and/or sequences of information of any complexity.
************************************
The Capacity of the Kanerva Associative Memory is Exponential
Philip Chou
The  capacity  of  an  associative  memory  is  defined  as  the  maximum 
************************************
The Sigmoid Nonlinearity in Prepyriform Cortex
Frank Eeckman
Requests for name changes in the electronic proceedings will be accepted with no questions asked.  However name changes may cause bibliographic tracking issues.  Authors are asked to consider this carefully and discuss it with their co-authors prior to requesting a name change in the electronic proceedings.
************************************
Probabilistic Characterization of Neural Model Computations
Richard Golden
Information  retrieval  in  a  neural  network  is  viewed  as  a  procedure  in 
************************************
Learning in Networks of Nondeterministic Adaptive Logic Elements
Richard Windecker
from
************************************
HIGH DENSITY ASSOCIATIVE MEMORIES
Amir Dembo, Ofer Zeitouni
from a description of desired properties 
************************************
A Mean Field Theory of Layer IV of Visual Cortex and Its Application to Artificial Neural Networks
Christopher Scofield
A  single  cell  theory  for  the  development  of  selectivity  and  ocular  dominance  in  visual  cortex  has  been  presented  previously  by  Bienenstock,  Cooper  and  Munrol.  This  has  been  extended  to  a  network  applicable  to  layer  IV  of  visual  cortex2 .  In  this  paper  we  present  a  mean  field  approximation  that  captures  in  a  fairly  transparent  manner  the  quantitative,  results  of  the  network  theory.  Finally,  we  consider  the  application  of  this  theory  to  artificial  neural  networks  and  show  that  a  significant  reduction  in  architectural  complexity  is  possible. 
************************************
Neural Networks for Template Matching: Application to Real-Time Classification of the Action Potentials of Real Neurons
James Bower, Yiu-Fai Wong, Jashojiban Banik
Much  experimental study  of  real  neural  networks  relies  on  the  proper  classification  of 
************************************
Capacity for Patterns and Sequences in Kanerva's SDM as Compared to Other Associative Memory Models
James Keeler
The  information  capacity  of Kanerva's  Sparse,  Distributed Memory  (SDM)  and  Hopfield-type  neural networks  is  investigated.  Under  the  approximations  used here,  it  is shown  that  the  to(cid:173) tal  information  stored in  these  systems  is proportional  to  the  number  connections  in  the  net(cid:173) work.  The  proportionality  constant  is  the  same  for  the  SDM  and  HopJreld-type  models  in(cid:173) dependent  of  the  particular  model,  or  the  order  of  the  model.  The  approximations  are  checked  numerically.  This  same  analysis  can  be  used  to  show  that  the  SDM  can  store  se(cid:173) quences  of spatiotemporal patterns,  and  the  addition  of time-delayed  connections  allows  the  retrieval  of context  dependent  temporal  patterns.  A  minor  modification  of the  SDM  can  be  used to store correlated patterns.
************************************
The Connectivity Analysis of Simple Association
Dan Hammerstrom
The efficient realization, using current silicon technology, of Very Large Connection  Networks (VLCN) with more than a billion connections requires that these networks exhibit  a high degree of communication locality. Real neural networks exhibit significant locality,  yet most connectionist/neural network models have little. In this paper, the connectivity  requirements of a simple associative network are analyzed using communication theory.  Several techniques based on communication theory are presented that improve the robust(cid:173) ness of the network in the face of sparse, local interconnect structures. Also discussed are  some potential problems when information is distributed too widely.
************************************
Performance Measures for Associative Memories that Learn and Forget
Anthony Kuh
Recently,  many  modifications  to  the  McCulloch/Pitts  model  have  been  proposed  where  both  learning  and  forgetting  occur.  Given  that  the  network  never saturates  (ceases  to  function  effectively  due  to  an  overload  of  information),  the  learning  updates  can  con(cid:173) tinue indefinitely.  For these networks,  we  need  to introduce  performance  measmes in  addi(cid:173) tion  to  the  information  capacity  to  evaluate  the  different  networks.  We  mathematically  define  quantities such  as  the  plasticity  of  a  network,  the  efficacy  of an  information  vector,  and the  probability  of network  saturation.  From  these  quantities  we  analytically  compare  different networks.
************************************
Centric Models of the Orientation Map in Primary Visual Cortex
William Baxter, Bruce Dow
In  the  visual  cortex  of  the  monkey  the  horizontal  organization  of  the  preferred  orientations of orientation-selective  cells  follows  two opposing  rules:  1) neighbors  tend  to  have similar orientation preferences, and  2) many different orientations are  observed  in a  local  region.  Several  orientation  models  which satisfy these  constraints are found  to  differ  in  the spacing  and  the  topological  index  of their singularities.  Using  the  rate  of orientation change as  a  measure,  the  models  are  compared to published experimental  results.
************************************
A Computer Simulation of Olfactory Cortex with Functional Implications for Storage and Retrieval of Olfactory Information
James Bower, Matthew Wilson
Based  on  anatomical  and  physiological  data,  we  have  developed  a  computer  simulation  of piri(cid:173) form  (olfactory)  cortex  which  is  capable  of reproducing  spatial  and  temporal  patterns  of actual  cortical  activity  under  a  variety  of conditions.  Using  a  simple  Hebb-type  learning  rule  in  conjunc(cid:173) tion  with  the  cortical  dynamics  which  emerge  from  the  anatomical  and  physiological  organiza(cid:173) tion  of  the  model,  the  simulations  are  capable  of establishing  cortical  representations  for  differ(cid:173) ent  input  patterns.  The  basis  of  these  representations  lies  in  the  interaction  of  sparsely  distribut(cid:173) ed,  highly  divergent/convergent  interconnections  between  modeled  neurons.  We  have  shown  that  different  representations  can  be  stored  with  minimal  interference.  and  that  following  learning  these  representations  are  resistant  to  input  degradation,  allowing  reconstruction  of  a  representa(cid:173) tion  following  only  a  partial  presentation  of  an  original  training  stimulus.  Further,  we  have  demonstrated  that  the  degree  of  overlap  of  cortical  representations  for  different  stimuli  can  also  be  modulated.  For  instance  similar  input  patterns  can  be  induced  to generate  distinct  cortical  representations  (discrimination).  while  dissimilar  inputs  can  be  induced  to  generate  overlapping  representations  (accommodation).  Both  features  are  presumably  important  in  classifying  olfacto(cid:173) ry stimuli.
************************************
Towards an Organizing Principle for a Layered Perceptual Network
Ralph Linsker
An information-theoretic optimization principle is  proposed for  the development  of  each  processing  stage  of  a  multilayered  perceptual  network.  This  principle  of  "maximum information preservation"  states that the signal transformation that is to be  realized at each stage is one that maximizes the information that the output signal values  (from that stage) convey about the input signals values (to that stage), subject to certain  constraints and in  the presence of processing noise.  The quantity being maximized is  a  Shannon information rate.  I provide motivation for this principle and -- for some simple  model cases -- derive some of its consequences, discuss an algorithmic implementation,  and  show  how  the  principle  may  lead  to  biologically  relevant  neural  architectural  features  such  as  topographic  maps,  map  distortions,  orientation  selectivity,  and  extraction of spatial and temporal signal correlations.  A  possible  connection between  this  information-theoretic principle  and  a  principle  of minimum  entropy production in  nonequilibrium thermodynamics is suggested.
************************************
A Trellis-Structured Neural Network
Thomas Petsche, Bradley Dickinson
We have developed a neural network which consists of cooperatively inter(cid:173)
************************************
Supervised Learning of Probability Distributions by Neural Networks
Eric Baum, Frank Wilczek
We propose that the back propagation algorithm for super(cid:173)
************************************
Stochastic Learning Networks and their Electronic Implementation
Joshua Alspector, Robert Allen, Victor Hu, Srinagesh Satyanarayana
Requests for name changes in the electronic proceedings will be accepted with no questions asked.  However name changes may cause bibliographic tracking issues.  Authors are asked to consider this carefully and discuss it with their co-authors prior to requesting a name change in the electronic proceedings.
************************************
Connecting to the Past
Bruce MacDonald
Recently  there  has  been  renewed  interest  in  neural-like  processing  systems,  evidenced  for  ex(cid:173) ample in  the two volumes  Parallel Distributed Processing edited by Rumelhart and McClelland,  and  discussed  as  parallel  distributed  systems,  connectionist  models,  neural  nets,  value  passing  systems  and  multiple  context  systems.  Dissatisfaction  with  symbolic  manipulation  paradigms  for  artificial  intelligence seems  partly  responsible  for  this  attention, encouraged by  the promise  of massively  parallel  systems  implemented  in  hardware.  This  paper  relates  simple  neural-like  systems  based  on  multiple  context  to  some  other  well-known  formalisms-namely  production  systems, k-Iength sequence prediction, finite-state  machines and Turing machines-and presents  earlier sequence  prediction  results  in  a  new  light.
************************************
PARTITIONING OF SENSORY DATA BY A CORTICAL NETWORK
Richard Granger, Jose Ambros-Ingerson, Howard Henry, Gary Lynch
Requests for name changes in the electronic proceedings will be accepted with no questions asked.  However name changes may cause bibliographic tracking issues.  Authors are asked to consider this carefully and discuss it with their co-authors prior to requesting a name change in the electronic proceedings.
************************************
A Dynamical Approach to Temporal Pattern Processing
W. Stornetta, Tad Hogg, Bernardo Huberman
Recognizing  patterns  with  temporal  context  is  important for  such  tasks  as  speech  recognition,  motion  detection  and  signature  verification.  We  propose  an  architecture  in  which  time  serves as its  own representation, and temporal context is encoded in the state of the  nodes. We contrast this with the approach of replicating portions of the  architecture to represent time. 
************************************
Minkowski-r Back-Propagation: Learning in Connectionist Models with Non-Euclidian Error Signals
Stephen Hanson, David Burr
Many connectionist learning models are implemented using a gradient descent  in a least squares error function of the output and teacher signal.  The present model  Fneralizes. in particular. back-propagation [1]  by using Minkowski-r power metrics.  For  small  r's  a  "city-block"  error  metric  is  approximated  and  for  large  r's  the  "maximum" or "supremum"  metric is  approached.  while  for r=2  the  standard  back(cid:173) propagation  model  results.  An  implementation  of Minkowski-r back-propagation  is  described.  and  several  experiments  are  done  which  show  that  different values  of r  may be desirable for various purposes. Different r values may be appropriate for the  reduction  of  the  effects  of outliers  (noise).  modeling  the  input  space  with  more  compact clusters. or modeling  the statistics of a particular domain more naturally or  in a way that may be more perceptually or psychologically meaningful (e.g. speech or  vision).
************************************
Analysis and Comparison of Different Learning Algorithms for Pattern Association Problems
J. Bernasconi
We 
************************************
